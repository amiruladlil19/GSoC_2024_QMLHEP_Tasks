{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0946e03a-c9c8-4321-b65f-1aa950094b73",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7e30bf-263c-4e99-9116-8683c90edf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 15:12:07.741093: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-02 15:12:07.748887: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-02 15:12:07.777344: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 15:12:07.777367: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 15:12:07.777384: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 15:12:07.782955: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-02 15:12:07.783330: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 15:12:08.490106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/am/Documents/VirtualEnv/penn_venv/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Define a function to change the MNIST label dimension\n",
    "def change_to_right(wrong_labels):\n",
    "    right_labels=[]\n",
    "    for x in wrong_labels:\n",
    "        for i in range(0,len(wrong_labels[0])):\n",
    "            if x[i]==1:\n",
    "                right_labels.append(i)\n",
    "    return right_labels\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Convert class vectors to right format\n",
    "y_train = tf.convert_to_tensor(np.array(change_to_right(keras.utils.to_categorical(y_train, num_classes))))\n",
    "y_test = tf.convert_to_tensor(np.array(change_to_right(keras.utils.to_categorical(y_test, num_classes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00077a1-40be-4b57-8d9b-f9bf23227491",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab72eeb-0da6-4ae8-aec1-eebe2aad9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 2000\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f593df1-e2c4-4ac7-9a77-04a579e2ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 56#28  # We'll resize input images to this size\n",
    "patch_size = 28#14  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 192#96\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 16\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4646d7-8c2c-474a-aca0-1dbdec9f8926",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f599d6-80fc-484c-808b-cd10453eac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        #layers.Resizing(image_size, image_size),\n",
    "        #layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4313ba76-51a9-4039-887a-49e9af5bd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0525f-6908-45e5-b62c-56e7739d6e7a",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6c7c74-c2af-40b6-8021-aa5eac0a8364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "27/27 [==============================] - 79s 3s/step - loss: 1.5722 - accuracy: 0.5735 - top-5-accuracy: 0.8984 - val_loss: 0.3390 - val_accuracy: 0.8973 - val_top-5-accuracy: 0.9953\n",
      "Epoch 2/30\n",
      "27/27 [==============================] - 71s 3s/step - loss: 0.4352 - accuracy: 0.8665 - top-5-accuracy: 0.9927 - val_loss: 0.1896 - val_accuracy: 0.9427 - val_top-5-accuracy: 0.9978\n",
      "Epoch 3/30\n",
      "27/27 [==============================] - 70s 3s/step - loss: 0.2845 - accuracy: 0.9148 - top-5-accuracy: 0.9960 - val_loss: 0.1393 - val_accuracy: 0.9610 - val_top-5-accuracy: 0.9975\n",
      "Epoch 4/30\n",
      "27/27 [==============================] - 70s 3s/step - loss: 0.2171 - accuracy: 0.9346 - top-5-accuracy: 0.9973 - val_loss: 0.1142 - val_accuracy: 0.9672 - val_top-5-accuracy: 0.9978\n",
      "Epoch 5/30\n",
      "27/27 [==============================] - 70s 3s/step - loss: 0.1832 - accuracy: 0.9459 - top-5-accuracy: 0.9978 - val_loss: 0.0997 - val_accuracy: 0.9723 - val_top-5-accuracy: 0.9983\n",
      "Epoch 6/30\n",
      "27/27 [==============================] - 70s 3s/step - loss: 0.1584 - accuracy: 0.9533 - top-5-accuracy: 0.9984 - val_loss: 0.0946 - val_accuracy: 0.9755 - val_top-5-accuracy: 0.9980\n",
      "Epoch 7/30\n",
      "27/27 [==============================] - 71s 3s/step - loss: 0.1322 - accuracy: 0.9593 - top-5-accuracy: 0.9989 - val_loss: 0.0814 - val_accuracy: 0.9773 - val_top-5-accuracy: 0.9987\n",
      "Epoch 8/30\n",
      "27/27 [==============================] - 72s 3s/step - loss: 0.1172 - accuracy: 0.9642 - top-5-accuracy: 0.9992 - val_loss: 0.0809 - val_accuracy: 0.9798 - val_top-5-accuracy: 0.9980\n",
      "Epoch 9/30\n",
      "27/27 [==============================] - 64s 2s/step - loss: 0.1067 - accuracy: 0.9678 - top-5-accuracy: 0.9990 - val_loss: 0.0765 - val_accuracy: 0.9788 - val_top-5-accuracy: 0.9985\n",
      "Epoch 10/30\n",
      "27/27 [==============================] - 67s 2s/step - loss: 0.1041 - accuracy: 0.9690 - top-5-accuracy: 0.9993 - val_loss: 0.0770 - val_accuracy: 0.9793 - val_top-5-accuracy: 0.9985\n",
      "Epoch 11/30\n",
      "27/27 [==============================] - 67s 2s/step - loss: 0.0930 - accuracy: 0.9722 - top-5-accuracy: 0.9994 - val_loss: 0.0759 - val_accuracy: 0.9790 - val_top-5-accuracy: 0.9983\n",
      "Epoch 12/30\n",
      "27/27 [==============================] - 66s 2s/step - loss: 0.0910 - accuracy: 0.9730 - top-5-accuracy: 0.9993 - val_loss: 0.0652 - val_accuracy: 0.9840 - val_top-5-accuracy: 0.9982\n",
      "Epoch 13/30\n",
      "27/27 [==============================] - 66s 2s/step - loss: 0.0812 - accuracy: 0.9759 - top-5-accuracy: 0.9995 - val_loss: 0.0612 - val_accuracy: 0.9833 - val_top-5-accuracy: 0.9985\n",
      "Epoch 14/30\n",
      "27/27 [==============================] - 66s 2s/step - loss: 0.0761 - accuracy: 0.9770 - top-5-accuracy: 0.9995 - val_loss: 0.0731 - val_accuracy: 0.9820 - val_top-5-accuracy: 0.9985\n",
      "Epoch 15/30\n",
      "27/27 [==============================] - 66s 2s/step - loss: 0.0721 - accuracy: 0.9774 - top-5-accuracy: 0.9996 - val_loss: 0.0613 - val_accuracy: 0.9837 - val_top-5-accuracy: 0.9987\n",
      "Epoch 16/30\n",
      "27/27 [==============================] - 65s 2s/step - loss: 0.0671 - accuracy: 0.9798 - top-5-accuracy: 0.9994 - val_loss: 0.0630 - val_accuracy: 0.9830 - val_top-5-accuracy: 0.9987\n",
      "Epoch 17/30\n",
      "27/27 [==============================] - 65s 2s/step - loss: 0.0638 - accuracy: 0.9804 - top-5-accuracy: 0.9997 - val_loss: 0.0698 - val_accuracy: 0.9837 - val_top-5-accuracy: 0.9983\n",
      "Epoch 18/30\n",
      "27/27 [==============================] - 64s 2s/step - loss: 0.0612 - accuracy: 0.9820 - top-5-accuracy: 0.9998 - val_loss: 0.0610 - val_accuracy: 0.9837 - val_top-5-accuracy: 0.9987\n",
      "Epoch 19/30\n",
      "27/27 [==============================] - 65s 2s/step - loss: 0.0595 - accuracy: 0.9817 - top-5-accuracy: 0.9998 - val_loss: 0.0637 - val_accuracy: 0.9838 - val_top-5-accuracy: 0.9985\n",
      "Epoch 20/30\n",
      "27/27 [==============================] - 66s 2s/step - loss: 0.0586 - accuracy: 0.9825 - top-5-accuracy: 0.9996 - val_loss: 0.0621 - val_accuracy: 0.9860 - val_top-5-accuracy: 0.9985\n",
      "Epoch 21/30\n",
      "27/27 [==============================] - 65s 2s/step - loss: 0.0564 - accuracy: 0.9831 - top-5-accuracy: 0.9998 - val_loss: 0.0698 - val_accuracy: 0.9830 - val_top-5-accuracy: 0.9982\n",
      "Epoch 22/30\n",
      "27/27 [==============================] - 64s 2s/step - loss: 0.0547 - accuracy: 0.9838 - top-5-accuracy: 0.9999 - val_loss: 0.0613 - val_accuracy: 0.9855 - val_top-5-accuracy: 0.9985\n",
      "Epoch 23/30\n",
      "27/27 [==============================] - 64s 2s/step - loss: 0.0493 - accuracy: 0.9853 - top-5-accuracy: 0.9998 - val_loss: 0.0613 - val_accuracy: 0.9852 - val_top-5-accuracy: 0.9983\n",
      "Epoch 24/30\n",
      "27/27 [==============================] - 66s 2s/step - loss: 0.0478 - accuracy: 0.9855 - top-5-accuracy: 0.9998 - val_loss: 0.0659 - val_accuracy: 0.9847 - val_top-5-accuracy: 0.9983\n",
      "Epoch 25/30\n",
      "27/27 [==============================] - 68s 3s/step - loss: 0.0455 - accuracy: 0.9864 - top-5-accuracy: 0.9998 - val_loss: 0.0559 - val_accuracy: 0.9877 - val_top-5-accuracy: 0.9983\n",
      "Epoch 26/30\n",
      "27/27 [==============================] - 68s 3s/step - loss: 0.0454 - accuracy: 0.9859 - top-5-accuracy: 0.9999 - val_loss: 0.0621 - val_accuracy: 0.9853 - val_top-5-accuracy: 0.9992\n",
      "Epoch 27/30\n",
      "27/27 [==============================] - 68s 3s/step - loss: 0.0471 - accuracy: 0.9855 - top-5-accuracy: 0.9998 - val_loss: 0.0640 - val_accuracy: 0.9843 - val_top-5-accuracy: 0.9985\n",
      "Epoch 28/30\n",
      "27/27 [==============================] - 68s 3s/step - loss: 0.0414 - accuracy: 0.9874 - top-5-accuracy: 0.9999 - val_loss: 0.0605 - val_accuracy: 0.9863 - val_top-5-accuracy: 0.9987\n",
      "Epoch 29/30\n",
      "27/27 [==============================] - 68s 3s/step - loss: 0.0439 - accuracy: 0.9865 - top-5-accuracy: 0.9999 - val_loss: 0.0613 - val_accuracy: 0.9858 - val_top-5-accuracy: 0.9987\n",
      "Epoch 30/30\n",
      "27/27 [==============================] - 69s 3s/step - loss: 0.0406 - accuracy: 0.9876 - top-5-accuracy: 0.9999 - val_loss: 0.0650 - val_accuracy: 0.9865 - val_top-5-accuracy: 0.9988\n",
      "313/313 [==============================] - 7s 18ms/step - loss: 0.0713 - accuracy: 0.9842 - top-5-accuracy: 0.9996\n",
      "Test accuracy: 98.42%\n",
      "Test top 5 accuracy: 99.96%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff9cde-1077-436c-ba62-5eb31490c9f9",
   "metadata": {},
   "source": [
    "## Possibility of Quantum Vision Transformer\n",
    "\n",
    "The vision transformer might be enhanced by employing quantum architecture to form the quantum transformers. The quantum transformers are based on compound matrices, and can be built by using shallow quantum circuit. There are three already proposed quantum transformers, they are called as the orthogonal patch-wise neural network, quantum orthogonal transformer, and quantum compound transformer. \n",
    "\n",
    "### Orthogonal patch-wise Neural Network\n",
    "\n",
    "This architecture is a transformer with a trivial attention mechanism, this is because each patch only pays attention to itself. Each input patch is multiplied by the same trainable matrix $V$ and one circuit per patch is used. There are $N = d$ qubits in each circuit and a quantum state with a vector data loader is used to encode each patch $x_i$. The multiplication of each patch with the matrix $V$ is done by a quantum orthogonal layer. Therefore, the output of each circuit is a quantum state encoding $Vx_i$\n",
    "\n",
    "### Quantum Orthogonal Transformer\n",
    "\n",
    "In this architecture, each attention coefficient $A_{ij} = x_i^TWx_j$ is firstly calculated; Each patch $x_j$ is loaded into the circuit with a vector loader followed by a trainable quantum orthogonal layer $W$, and then an inverse data loader of $x_i$ is applied. The result is the probability of measuring $1$ on the first qubit is $|x_i^TWx_j|^2 = A_{ij}^2$.\n",
    "\n",
    "### Quantum Compound Transformer\n",
    "\n",
    "The compound transformer works by firstly loading all patches with the same weight, applying superposition to each patch, and then applying orthogonal layer. By doing this, the features from each patch can be extracted and the patches can be re-weighted. The output will be computed as a weighted sum of the features extracted from all patches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f6596-5000-4cb7-9f9f-9a7cfe7719e0",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Gros, Claudius. \"Visual Transformer and the MNIST Dataset.\" Institute for Theoretical Physics, Goethe University Frankfurt, 2023, https://itp.uni-frankfurt.de/~gros/StudentProjects/WS22_23_VisualTransformer/ <br/>\n",
    "2. Kerenidis, Iordanis, et al. \"Quantum Vision Transformers.\" (2022).\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c05da-f2fa-45d2-ba07-6ce52ada8696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane_kernel",
   "language": "python",
   "name": "pennylane_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
